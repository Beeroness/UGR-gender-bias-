# UGR-gender-bias-

Category	Bias Type	Definition	Illustrative Example
Data-Driven	Historical Bias	Data reflects past societal prejudices, which the model learns and perpetuates.	
A hiring algorithm trained on past data from a male-dominated tech industry learns to penalize female candidates.   

Selection Bias	Training data is not representative of the target population.	
Facial recognition models trained primarily on light-skinned faces show high error rates for dark-skinned women.   

Measurement Bias	Data systematically differs from the true variables of interest, often due to poor proxies.	
A healthcare algorithm uses cost as a proxy for need, disadvantaging Black patients who historically spend less on care.   

Model-Driven	Proxy Discrimination	The model uses seemingly neutral attributes (e.g., ZIP code) that are highly correlated with protected attributes (e.g., race).	
A loan algorithm denies applicants from a specific ZIP code, which indirectly discriminates against a racial minority group concentrated there.   

Amplification Bias	The model exacerbates existing biases found in the data, creating a feedback loop.	
An AI model not only learns human stereotypes but makes them more extreme in its outputs, which can then influence users to become more biased.   

Human-Cognitive	Confirmation Bias	The tendency to favor information that confirms pre-existing beliefs, influencing model design or interpretation.	
A developer retrains a model repeatedly until it confirms their personal hypothesis that a certain dog breed is aggressive.   

In-group Bias	A preference for members of one's own group, leading to biased data curation or feature engineering.	
Developers of a resume-screening tool are predisposed to believe applicants from their alma mater are more qualified.   

Section 3: The Harms of Automation: Case Studi


# Thoughts/Brain Dumps
- Under-representation/Stereotypes/
- lack of resources
- 22% Female Gender in AI dev
- Gender Fluid non-binary
- Grouping (since we used statistics to model these LLMs (removing outliers)  <--- this has only been something that did not sit well since the first time I heard of it. I have had professors explicitly say when designing our models/solutions, "we do not care about oiutliers, throw them away"  
- 


